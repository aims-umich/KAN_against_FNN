{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('chf_train.csv')\n",
    "valid_data = pd.read_csv('chf_valid.csv')\n",
    "\n",
    "# Split inputs (X) and outputs (y)\n",
    "x_train = train_data.iloc[:, :6].values\n",
    "y_train = train_data.iloc[:, 6].values.reshape(-1, 1)\n",
    "x_test = valid_data.iloc[:, :6].values\n",
    "y_test = valid_data.iloc[:, 6].values.reshape(-1, 1)\n",
    "\n",
    "# Scale data\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_train = x_scaler.fit_transform(x_train)\n",
    "X_test = x_scaler.transform(x_test)\n",
    "Y_train = y_scaler.fit_transform(y_train)\n",
    "Y_test = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(300, activation='relu', input_shape=(6,)),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2012/2012 [==============================] - 9s 4ms/step - loss: 9.9489e-04 - val_loss: 3.5637e-04\n",
      "Epoch 2/100\n",
      "2012/2012 [==============================] - 5s 2ms/step - loss: 4.2013e-04 - val_loss: 3.1072e-04\n",
      "Epoch 3/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.2054e-04 - val_loss: 1.8117e-04\n",
      "Epoch 4/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 2.8508e-04 - val_loss: 2.0472e-04\n",
      "Epoch 5/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 2.2394e-04 - val_loss: 1.1691e-04\n",
      "Epoch 6/100\n",
      "2012/2012 [==============================] - 5s 2ms/step - loss: 2.0616e-04 - val_loss: 8.0310e-05\n",
      "Epoch 7/100\n",
      "2012/2012 [==============================] - 5s 2ms/step - loss: 1.9409e-04 - val_loss: 1.1181e-04\n",
      "Epoch 8/100\n",
      "2012/2012 [==============================] - 5s 2ms/step - loss: 1.7844e-04 - val_loss: 3.3561e-04\n",
      "Epoch 9/100\n",
      "2012/2012 [==============================] - 5s 2ms/step - loss: 1.5395e-04 - val_loss: 1.2261e-04\n",
      "Epoch 10/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.5831e-04 - val_loss: 7.2274e-05\n",
      "Epoch 11/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.5065e-04 - val_loss: 1.7043e-04\n",
      "Epoch 12/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.1533e-04 - val_loss: 6.4592e-05\n",
      "Epoch 13/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.2771e-04 - val_loss: 1.1053e-04\n",
      "Epoch 14/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.2789e-04 - val_loss: 5.8014e-05\n",
      "Epoch 15/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.0509e-04 - val_loss: 6.7900e-05\n",
      "Epoch 16/100\n",
      "2012/2012 [==============================] - 5s 3ms/step - loss: 1.0662e-04 - val_loss: 8.0462e-05\n",
      "Epoch 17/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 1.1756e-04 - val_loss: 4.3199e-05\n",
      "Epoch 18/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 9.7150e-05 - val_loss: 2.1965e-04\n",
      "Epoch 19/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 9.8199e-05 - val_loss: 5.1200e-05\n",
      "Epoch 20/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 8.6012e-05 - val_loss: 4.0371e-05\n",
      "Epoch 21/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 9.7506e-05 - val_loss: 7.1463e-05\n",
      "Epoch 22/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 7.8994e-05 - val_loss: 5.1600e-05\n",
      "Epoch 23/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 8.1297e-05 - val_loss: 8.3159e-05\n",
      "Epoch 24/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 8.3293e-05 - val_loss: 5.0140e-05\n",
      "Epoch 25/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 8.0834e-05 - val_loss: 7.3600e-05\n",
      "Epoch 26/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 8.0286e-05 - val_loss: 3.8820e-05\n",
      "Epoch 27/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 7.5416e-05 - val_loss: 5.1344e-05\n",
      "Epoch 28/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 6.7636e-05 - val_loss: 9.4104e-05\n",
      "Epoch 29/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 7.9726e-05 - val_loss: 3.5180e-05\n",
      "Epoch 30/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 7.5143e-05 - val_loss: 4.9515e-05\n",
      "Epoch 31/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.3997e-05 - val_loss: 5.8971e-05\n",
      "Epoch 32/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 8.2624e-05 - val_loss: 3.9634e-05\n",
      "Epoch 33/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 6.4867e-05 - val_loss: 3.5320e-05\n",
      "Epoch 34/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 7.5471e-05 - val_loss: 6.0635e-05\n",
      "Epoch 35/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.6208e-05 - val_loss: 3.1177e-05\n",
      "Epoch 36/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.8172e-05 - val_loss: 3.2684e-05\n",
      "Epoch 37/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.9965e-05 - val_loss: 3.6649e-05\n",
      "Epoch 38/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.6345e-05 - val_loss: 7.4207e-05\n",
      "Epoch 39/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.9404e-05 - val_loss: 3.1161e-05\n",
      "Epoch 40/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.9989e-05 - val_loss: 4.7087e-05\n",
      "Epoch 41/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.3618e-05 - val_loss: 2.9936e-05\n",
      "Epoch 42/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.8408e-05 - val_loss: 2.6962e-05\n",
      "Epoch 43/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.3605e-05 - val_loss: 4.2717e-05\n",
      "Epoch 44/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.4629e-05 - val_loss: 1.2778e-04\n",
      "Epoch 45/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.6535e-05 - val_loss: 2.7308e-05\n",
      "Epoch 46/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.5804e-05 - val_loss: 2.4094e-05\n",
      "Epoch 47/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.8871e-05 - val_loss: 3.2572e-05\n",
      "Epoch 48/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.1890e-05 - val_loss: 1.9884e-05\n",
      "Epoch 49/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.2230e-05 - val_loss: 2.5222e-05\n",
      "Epoch 50/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.0722e-05 - val_loss: 2.7409e-05\n",
      "Epoch 51/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.9236e-05 - val_loss: 4.6741e-05\n",
      "Epoch 52/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.8893e-05 - val_loss: 2.1129e-05\n",
      "Epoch 53/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.9310e-05 - val_loss: 2.4036e-05\n",
      "Epoch 54/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.4956e-05 - val_loss: 3.7018e-05\n",
      "Epoch 55/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.7750e-05 - val_loss: 3.5892e-05\n",
      "Epoch 56/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.1502e-05 - val_loss: 2.2267e-05\n",
      "Epoch 57/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.0774e-05 - val_loss: 2.5455e-05\n",
      "Epoch 58/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.8971e-05 - val_loss: 3.1752e-05\n",
      "Epoch 59/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.6104e-05 - val_loss: 1.8616e-05\n",
      "Epoch 60/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.0794e-05 - val_loss: 3.2298e-05\n",
      "Epoch 61/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.1856e-05 - val_loss: 2.6714e-05\n",
      "Epoch 62/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.3529e-05 - val_loss: 3.7723e-05\n",
      "Epoch 63/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.2357e-05 - val_loss: 2.7592e-05\n",
      "Epoch 64/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.6112e-05 - val_loss: 5.6998e-05\n",
      "Epoch 65/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.6345e-05 - val_loss: 5.2861e-05\n",
      "Epoch 66/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 5.0061e-05 - val_loss: 4.9894e-05\n",
      "Epoch 67/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.1831e-05 - val_loss: 4.9167e-05\n",
      "Epoch 68/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.3943e-05 - val_loss: 2.7396e-05\n",
      "Epoch 69/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.0329e-05 - val_loss: 2.3046e-05\n",
      "Epoch 70/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.2340e-05 - val_loss: 2.1536e-05\n",
      "Epoch 71/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.6336e-05 - val_loss: 1.6761e-05\n",
      "Epoch 72/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.6690e-05 - val_loss: 2.6393e-05\n",
      "Epoch 73/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.8212e-05 - val_loss: 1.8753e-05\n",
      "Epoch 74/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.1926e-05 - val_loss: 5.4752e-05\n",
      "Epoch 75/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.6715e-05 - val_loss: 6.1252e-05\n",
      "Epoch 76/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.0552e-05 - val_loss: 2.3201e-05\n",
      "Epoch 77/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.5818e-05 - val_loss: 3.0037e-05\n",
      "Epoch 78/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.0521e-05 - val_loss: 1.5844e-05\n",
      "Epoch 79/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.2447e-05 - val_loss: 2.2424e-05\n",
      "Epoch 80/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.8481e-05 - val_loss: 5.1614e-05\n",
      "Epoch 81/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.6813e-05 - val_loss: 3.5242e-05\n",
      "Epoch 82/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.9115e-05 - val_loss: 2.7086e-05\n",
      "Epoch 83/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 4.0054e-05 - val_loss: 1.4159e-05\n",
      "Epoch 84/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.5834e-05 - val_loss: 1.9608e-05\n",
      "Epoch 85/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.9927e-05 - val_loss: 3.2981e-05\n",
      "Epoch 86/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.9556e-05 - val_loss: 1.3668e-05\n",
      "Epoch 87/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 4.3289e-05 - val_loss: 1.3857e-05\n",
      "Epoch 88/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.1549e-05 - val_loss: 3.6859e-05\n",
      "Epoch 89/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 3.7875e-05 - val_loss: 4.8907e-05\n",
      "Epoch 90/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 4.1368e-05 - val_loss: 1.2211e-05\n",
      "Epoch 91/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.2754e-05 - val_loss: 2.2168e-05\n",
      "Epoch 92/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 3.6266e-05 - val_loss: 2.7045e-05\n",
      "Epoch 93/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.7863e-05 - val_loss: 2.0637e-05\n",
      "Epoch 94/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.3161e-05 - val_loss: 4.3529e-05\n",
      "Epoch 95/100\n",
      "2012/2012 [==============================] - 3s 2ms/step - loss: 3.4047e-05 - val_loss: 3.8580e-05\n",
      "Epoch 96/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.4063e-05 - val_loss: 1.8556e-05\n",
      "Epoch 97/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.3675e-05 - val_loss: 2.7230e-05\n",
      "Epoch 98/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.1680e-05 - val_loss: 1.7276e-05\n",
      "Epoch 99/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.5451e-05 - val_loss: 1.1323e-05\n",
      "Epoch 100/100\n",
      "2012/2012 [==============================] - 4s 2ms/step - loss: 3.3855e-05 - val_loss: 1.3606e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 1ms/step\n",
      "Predictions exported to FNN_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_test, Y_test, and the model are already defined\n",
    "# Predict the output using the model\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Combine X_test, Y_test, and Y_test_pred into a single DataFrame\n",
    "data = np.hstack((X_test, Y_test.reshape(-1, 1), Y_test_pred.reshape(-1, 1)))\n",
    "columns = [f\"x{i+1}\" for i in range(X_test.shape[1])] + [\"y_test\", \"y_test_pred\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "output_file = \"FNN_predictions.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Predictions exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 1s 1ms/step\n",
      "168/168 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_scaled = model.predict(X_train)\n",
    "y_test_pred_scaled = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = y_scaler.inverse_transform(y_train_pred_scaled)\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "MAE: 31.7121, MAPE: 2.3992, MSE: 2768.4197, RMSE: 52.6158, RMSPE: 4.6512, R2: 0.9986\n",
      "Test Metrics:\n",
      "MAE: 30.7347, MAPE: 2.1062, MSE: 2376.1755, RMSE: 48.7460, RMSPE: 3.1870, R2: 0.9986\n"
     ]
    }
   ],
   "source": [
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mape_train = np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmspe_train = np.sqrt(np.mean(np.square((y_train - y_train_pred) / y_train))) * 100 \n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "rmspe_test = np.sqrt(np.mean(np.square((y_test - y_test_pred) / y_test))) * 100 \n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Metrics:\")\n",
    "print(f\"MAE: {mae_train:.4f}, MAPE: {mape_train:.4f}, MSE: {mse_train:.4f}, RMSE: {rmse_train:.4f}, RMSPE: {rmspe_train:.4f}, R2: {r2_train:.4f}\")\n",
    "\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"MAE: {mae_test:.4f}, MAPE: {mape_test:.4f}, MSE: {mse_test:.4f}, RMSE: {rmse_test:.4f}, RMSPE: {rmspe_test:.4f}, R2: {r2_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
